# --------------------------------common----------------------------#
collector.version=v1.0.0
# high availability option.
master_quorum = false
# http api interface. Users can use this api to monitor mongoshake.
full_sync.http_port = 9101
incr_sync.http_port = 9100
# profiling on net/http/profile
system_profile_port = 9200

## log
# global log level: debug, info, warning, error. lower level message will be filter
log.level = info
log.dir = E:\\mygit\\db-sync\\log
log.file = collector.log
log.flush = false

# full, incr, all
collector.mode=full

# -------------------------------source-----------------------------
# replication:
#collector.source.url=mysql://localhost:3306,localhost:3307?database=wyett&&charset=utf8mb4
# database info
collector.source.host=10.16.17.103:3306,10.16.17.107:3306
collector.source.user=wyett_rw
collector.source.password=jumpjump
collector.source.dbname=wyett
collector.source.charset=utf8

# connection pool
collector.source.pool.maxIdleConns=10
collector.source.pool.maxLifeTime=10
collector.source.pool.maxIdleTime=10
collector.source.pool.maxOpenConns=20

# collector proxy
#
collector.source.proxy.method=


## sync
# instance, with no administrator databases
# databases, database name list
# tables, table list
# seperator by comma
collector.sync.mode=instance
collector.sync.whitelist=wyett.\\.*
collector.sync.blacklist=wyett1\\.*

## filter
# range, contain, None
collector.filter.type=none

# -----------------------------destination--------------------------------
# rpc, kafka, rocketmq, direct, rabbitmq, replicate, standlone
collector.destination.mode=replicate
#collector.destination.url=a
collector.destination.mysql.host=10.16.17.103:3306,10.16.17.107:3306
collector.destination.mysql.user=wyett_rw
collector.destination.mysql.password=jumpjump
collector.destination.mysql.dbname=wyett
collector.destination.mysql.charset=utf8
#collectordestination.source.url=mysql://localhost:3306,localhost:3307?database=wyett&&charset=utf8mb4
# connection pool
collector.destination.pool.maxIdleConns=10
collector.destination.pool.maxLifeTime=10
collector.destination.pool.maxIdleTime=10
collector.destination.pool.maxOpenConns=20
# collector destination proxy
collector.destination.proxy.method=
# mq
collector.destination.mq.servers = 10.19.58.12:9876;10.19.58.14:9876;10.19.58.17:9876
collector.destination.mq.retries = 0
collector.destination.mq.batchSize = 16384
collector.destination.mq.maxRequestSize = 1048576
collector.destination.mq.lingerMs = 100
collector.destination.mq.bufferMemory = 33554432
collector.destination.mq.canalBatchSize = 50
collector.destination.mq.canalGetTimeout = 100
collector.destination.mq.flatMessage = true
collector.destination.mq.compressionType = none
collector.destination.mq.acks = all
collector.destination.mq.producerGroup = test
collector.destination.mq.accessChannel = local
collector.destination.mq.transaction = false

# ----------------------checkpoint------------------------------#
# save checkpoint into zookeeper
checkpoint.storage.url =
# checkpoint db's name.
checkpoint.storage.db = mongoshake
# checkpoint collection's name.
checkpoint.storage.collection = ckpt_default
# real checkpoint: the fetching oplog position.
checkpoint.start_position = 1970-01-01T00:00:00Z
# transform: fromDbName1.fromCollectionName1:toDbName1.toCollectionName1;fromDbName2:toDbName2
transform.namespace =

# ---------------------------------full.sync---------------------------------------#
# the number of collection concurrence
full_sync.reader.table_parallel = 6
# the number of document writer thread in each collection.
full_sync.reader.write_row_parallel = 8
# number of documents in a batch insert in a document concurrence
full_sync.reader.row_batch_size = 128
# number of documents reading in single reader thread.
# do not enable when the _id has more than one type: e.g., ObjectId, string.
full_sync.reader.read_row_count = 0
# drop the same name of collection in dest mongodb in full synchronization
full_sync.table_exist_drop = true
# create index option.
# none: do not create indexes.
# beforesync: create indexes when data sync finish in full sync stage.
# aftersync: create indexes when starting.
full_sync.create_index = none
# convert insert to update when duplicate key found
full_sync.executor.insert_on_dup_update = false
















#eof
